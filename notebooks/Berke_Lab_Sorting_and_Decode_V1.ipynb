{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb86657",
   "metadata": {},
   "source": [
    "# Berke Lab Spike Sorting and Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778bf96-740c-4e4b-a695-ed4385fc9b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "import spyglass.common as sgc\n",
    "import spyglass.spikesorting.v1 as sgs\n",
    "import spyglass.position as sgp\n",
    "from spyglass.common import Nwbfile\n",
    "from spyglass.utils.nwb_helper_fn import get_nwb_file\n",
    "\n",
    "# Make sure the session exists\n",
    "# nwb_file_name = \"IM-1594_20230726_.nwb\"\n",
    "nwb_file_name = \"IM-1478_20220726_.nwb\"\n",
    "\n",
    "# Fetch file create date and source version to make sure it's up to date\n",
    "nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n",
    "nwbf = get_nwb_file(nwb_file_abspath)\n",
    "print(f\"File created on {nwbf.file_create_date[0].strftime('%m/%d/%Y %H:%M:%S')}\")\n",
    "print(f\"Source script version {nwbf.source_script}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a62cd",
   "metadata": {},
   "source": [
    "Take a quick look at the parameters we are using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.spikesorting.analysis.v1.group import UnitSelectionParams\n",
    "from spyglass.decoding.v1.core import DecodingParameters\n",
    "\n",
    "# Preprocessing\n",
    "preproc_param_name = \"franklab_tetrode_hippocampus\"\n",
    "artifact_param_name = \"ampl_1000_z_30_prop_075_1ms\"\n",
    "# Sorting\n",
    "sorter = \"mountainsort4\"\n",
    "sorter_param_name = \"franklab_tetrode_hippocampus_30KHz\"\n",
    "# Curation\n",
    "waveform_param_name = \"default_not_whitened\"\n",
    "metric_param_name = \"franklab_default\"\n",
    "metric_curation_param_name = \"default\"\n",
    "# Decoding\n",
    "unit_filter_params_name = \"default_exclusion\"\n",
    "decoding_param_name = \"contfrag_sorted\"\n",
    "# Position (for decoding)\n",
    "trodes_pos_params_name = \"berke_double_led_500\"\n",
    "\n",
    "# Optionally print them all!\n",
    "review_params = True\n",
    "if review_params:\n",
    "    # Preprocessing\n",
    "    display(\n",
    "        (\n",
    "            sgs.SpikeSortingPreprocessingParameters()\n",
    "            & {\"preproc_param_name\": preproc_param_name}\n",
    "        ).fetch1()\n",
    "    )\n",
    "    display(\n",
    "        (\n",
    "            sgs.ArtifactDetectionParameters()\n",
    "            & {\"artifact_param_name\": artifact_param_name}\n",
    "        ).fetch1(\"artifact_params\")\n",
    "    )\n",
    "    # Sorting\n",
    "    display(\n",
    "        (\n",
    "            sgs.SpikeSorterParameters()\n",
    "            & {\"sorter\": sorter, \"sorter_param_name\": sorter_param_name}\n",
    "        ).fetch1()\n",
    "    )\n",
    "    # Curation\n",
    "    display(\n",
    "        (\n",
    "            sgs.WaveformParameters() & {\"waveform_param_name\": waveform_param_name}\n",
    "        ).fetch1()\n",
    "    )\n",
    "    display(\n",
    "        (sgs.MetricParameters() & {\"metric_param_name\": metric_param_name}).fetch1()\n",
    "    )\n",
    "    display(\n",
    "        (\n",
    "            sgs.MetricCurationParameters()\n",
    "            & {\"metric_curation_param_name\": metric_curation_param_name}\n",
    "        ).fetch1()\n",
    "    )\n",
    "    # Decoding\n",
    "    display(\n",
    "        (\n",
    "            UnitSelectionParams() & {\"unit_filter_params_name\": unit_filter_params_name}\n",
    "        ).fetch1()\n",
    "    )\n",
    "    display(\n",
    "        (DecodingParameters() & {\"decoding_param_name\": decoding_param_name}).fetch(\n",
    "            \"decoding_params\"\n",
    "        )\n",
    "    )\n",
    "    # Position (for decoding)\n",
    "    display(\n",
    "        (\n",
    "            sgp.v1.TrodesPosParams()\n",
    "            & {\"trodes_pos_params_name\": trodes_pos_params_name}\n",
    "        ).fetch1(\"params\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4eadd1",
   "metadata": {},
   "source": [
    "# First check out all existing entries for this nwb\n\nHelpful if we're halfway through running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ac69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.spikesorting.spikesorting_merge import SpikeSortingOutput\n",
    "from spyglass.spikesorting.analysis.v1.group import SortedSpikesGroup\n",
    "from spyglass.position import PositionOutput\n",
    "\n",
    "# Find all entries for this nwb in PositionOutput\n",
    "print(f\"Entries for {nwb_file_name} in PositionOutput.TrodesPosV1\")\n",
    "display(PositionOutput.TrodesPosV1 & {\"nwb_file_name\": nwb_file_name})\n",
    "\n",
    "# Find all entries for this nwb in SortGroup\n",
    "print(f\"Entries for {nwb_file_name} in sgs.SortGroup\")\n",
    "display((sgs.SortGroup & {\"nwb_file_name\": nwb_file_name}))\n",
    "\n",
    "# Find all entries for this nwb in SpikeSortingRecordingSelection\n",
    "print(\n",
    "    f\"Entries for {nwb_file_name} in sgs.SpikeSortingRecordingSelection (one per SortGroup, or more if we are trying multiple preprocessing params)\"\n",
    ")\n",
    "display(sgs.SpikeSortingRecordingSelection() & {\"nwb_file_name\": nwb_file_name})\n",
    "\n",
    "# Fetch the recording ids (there is one for each sort group)\n",
    "recording_ids = (\n",
    "    sgs.SpikeSortingRecordingSelection() & {\"nwb_file_name\": nwb_file_name}\n",
    ").fetch(\"KEY\")\n",
    "\n",
    "# Use the recording_ids to get the preprocessed recording for each\n",
    "print(f\"Entries for {nwb_file_name} in sgs.SpikeSortingRecording (one per SortGroup)\")\n",
    "display(sgs.SpikeSortingRecording() & recording_ids)\n",
    "\n",
    "# Get the detected artifact times for this recording\n",
    "print(\n",
    "    f\"Entries for {nwb_file_name} in sgs.ArtifactDetectionSelection (one per recording_id)\"\n",
    ")\n",
    "display(sgs.ArtifactDetectionSelection() & recording_ids)\n",
    "artifact_ids = (sgs.ArtifactDetectionSelection() & recording_ids).fetch(\"KEY\")\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in sgs.ArtifactDetection (one per recording_id)\")\n",
    "display(sgs.ArtifactDetection() & artifact_ids)\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in sgs.SpikeSortingSelection\")\n",
    "display(sgs.SpikeSortingSelection() & {\"nwb_file_name\": nwb_file_name})\n",
    "sorting_ids = (sgs.SpikeSortingSelection() & {\"nwb_file_name\": nwb_file_name}).fetch(\n",
    "    \"KEY\"\n",
    ")\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in sgs.SpikeSorting\")\n",
    "display(sgs.SpikeSorting() & sorting_ids)\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in sgs.CurationV1\")\n",
    "display(sgs.CurationV1() & sorting_ids)\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in sgs.MetricCurationSelection\")\n",
    "display(sgs.MetricCurationSelection() & sorting_ids)\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in sgs.SpikeSortingOutput\")\n",
    "merge_ids = SpikeSortingOutput().get_restricted_merge_ids(\n",
    "    key={\"nwb_file_name\": nwb_file_name}, as_dict=True\n",
    ")\n",
    "display(SpikeSortingOutput & merge_ids)\n",
    "\n",
    "print(f\"Entries for {nwb_file_name} in SortedSpikesGroup\")\n",
    "display(SortedSpikesGroup & {\"nwb_file_name\": nwb_file_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e390a71",
   "metadata": {},
   "source": [
    "## Define sort groups and extract recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3dfe2d-4645-44f9-b169-479292215afe",
   "metadata": {},
   "source": [
    "For now we generally sort electrodes on the same shank together. \n\n\nI have also written some custom functions that allow us to set SortGroups based on different attributes, so we may have way more `SortGroups` than shanks (if we started by one `SortGroup` per shank, then chose custom ones with different numbers). Generally when we define custom `SortGroups` we'll start from higher numbers to make it clear that they are separate from the ones auto-assigned by `sgs.SortGroup.set_group_by_shank` (e.g. if `set_group_by_shank` assigns groups 0-24 for 25 good shanks, we might choose to start our custom `SortGroups` at id 40 to leave a clear gap in between them)\n\n\nIf you do this (have multiple sets of `SortGroups`), remember to choose the actual sort groups you want and don't just use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269f6af-eb16-4551-b511-a264368c9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_sort_group_ids = (sgs.SortGroup & {\"nwb_file_name\": nwb_file_name}).fetch(\n    \"sort_group_id\"\n)\nprint(f\"All existing sort group ids for this nwb: {existing_sort_group_ids}\")\n\n# Quick check before we overwrite everything!\n# If no SortGroups exist yet, we generally start with setting them by shank.\nif existing_sort_group_ids.size == 0:\n    sgs.SortGroup.set_group_by_shank(nwb_file_name=nwb_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c8c61",
   "metadata": {},
   "source": [
    "Choose the sort group ids we actually want to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use all of them:\n# sort_group_ids = (sgs.SortGroup & {\"nwb_file_name\": nwb_file_name}).fetch(\"sort_group_id\")\n\n# Or pick a few:\n# NOTE: For IM-1478_20220726_.nwb I'm using SortGroups 1-25 (output by set_group_by_shank)\n# Yang-Sun uses custom groups with ids 40+ for clusterless decoding\nsort_group_ids = list(range(25))\n\nprint(f\"Using sort_group_ids: {sort_group_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae4b40",
   "metadata": {},
   "source": [
    "## Preprocessing\n\nFilter and reference the recording so that we isolate the spike band data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b307631-3cc5-4859-9e95-aeedf6a3de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and insert a key for each sort group / interval / parameters you want to sort\n\ngroup_keys = []\nfor sort_group_id in sort_group_ids:\n    key = {\n        \"nwb_file_name\": nwb_file_name,\n        \"sort_group_id\": sort_group_id,\n        \"interval_list_name\": \"00_r1\",\n        \"preproc_param_name\": preproc_param_name,\n        \"team_name\": \"Berke lab and friends\",\n    }\n    # Insert into the selection table\n    sgs.SpikeSortingRecordingSelection.insert_selection(key)\n\n    # Grab the primary key (recording_id) and add to our list so we can insert into SpikeSortingRecording\n    group_keys.append((sgs.SpikeSortingRecordingSelection & key).fetch1(\"KEY\"))\n\n# Look at everything we inserted!\ndisplay(sgs.SpikeSortingRecordingSelection & group_keys)\n\nprint(\"Group keys:\")\nprint(group_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01948666",
   "metadata": {},
   "source": [
    "Now call the `populate` method of `SpikeSortingRecording`. \n\nInstead of just calling `sgs.SpikeSortingRecording.populate(group_keys)` with all group_keys, we only populate the missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate SpikeSortingRecording for all group_keys (ignoring ones already populated)\n",
    "\n",
    "# Print the set of all possible group keys\n",
    "print(f\"There are {len(group_keys)} keys: {group_keys}\")\n",
    "\n",
    "# Get the set of already-populated keys in SpikeSortingRecording\n",
    "existing_keys = (sgs.SpikeSortingRecording & group_keys).fetch(\"KEY\", as_dict=True)\n",
    "print(\n",
    "    f\"There are {len(existing_keys)} already in in SpikeSortingRecording: {existing_keys}\"\n",
    ")\n",
    "\n",
    "# Find missing keys\n",
    "missing_keys = [key for key in group_keys if key not in existing_keys]\n",
    "print(f\"There are {len(missing_keys)} missing keys: {missing_keys}\")\n",
    "\n",
    "# Populate only missing entries\n",
    "if missing_keys:\n",
    "    sgs.SpikeSortingRecording().populate(missing_keys)\n",
    "else:\n",
    "    print(\"All group keys already populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c7ea3-9538-4fa9-890b-ee16cc18af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything worked!\ndisplay(sgs.SpikeSortingRecording() & group_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b379bd",
   "metadata": {},
   "source": [
    "## Plot raw ElectricalSeries directly from the NWB\n\nThis plots a chunk of `e_series = nwbf.acquisition.get('ElectricalSeries')` for each shank.\n\nNo preprocessing has happened at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time window\n",
    "start_time = 43\n",
    "duration = 5\n",
    "\n",
    "# Get RAW ElectricalSeries from this nwb\n",
    "e_series = nwbf.acquisition.get(\"ElectricalSeries\")\n",
    "timestamps = e_series.timestamps\n",
    "n_timestamps = len(timestamps)\n",
    "\n",
    "\n",
    "# Helper to find the first index >= target_time using binary search (because we have non-uniform timestamps)\n",
    "def find_index(target_time, left=0, right=n_timestamps - 1):\n",
    "    while left < right:\n",
    "        mid = (left + right) // 2\n",
    "        mid_val = timestamps[mid]\n",
    "        if mid_val < target_time:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid\n",
    "    return left\n",
    "\n",
    "\n",
    "# Find start and end indices using binary search (without loading all)\n",
    "start_idx = find_index(start_time)\n",
    "end_idx = find_index(start_time + duration)\n",
    "\n",
    "\n",
    "def safe_index(vector_data, indices):\n",
    "    \"\"\"Helper to index NWB VectorData columns (works around HDF5 ordering limits)\"\"\"\n",
    "    indices = np.asarray(indices)\n",
    "    order = np.argsort(indices)\n",
    "    sorted_vals = np.array(vector_data[indices[order]])\n",
    "    return sorted_vals[np.argsort(order)]\n",
    "\n",
    "\n",
    "t = timestamps[start_idx:end_idx]\n",
    "data_chunk = e_series.data[start_idx:end_idx, :]  # lazy slice, all channels\n",
    "electrode_table = nwbf.electrodes\n",
    "electrode_region = e_series.electrodes\n",
    "electrode_ids = electrode_region.data[:]  # indices into the electrode table\n",
    "\n",
    "shanks = safe_index(electrode_table[\"probe_shank\"], electrode_ids)\n",
    "electrodes = safe_index(electrode_table[\"probe_electrode\"], electrode_ids)\n",
    "electrode_names = safe_index(electrode_table[\"electrode_name\"], electrode_ids)\n",
    "open_ephys_names = safe_index(electrode_table[\"open_ephys_channel_str\"], electrode_ids)\n",
    "bad_channels = safe_index(electrode_table[\"bad_channel\"], electrode_ids)\n",
    "\n",
    "offset = 1000\n",
    "unique_shanks = np.unique(shanks)\n",
    "for shank in unique_shanks:\n",
    "    # Select and sort electrodes within this shank\n",
    "    shank_mask = shanks == shank\n",
    "    sort_idx = np.argsort(electrodes[shank_mask])\n",
    "    # Apply mask and sort\n",
    "    data_shank = data_chunk[:, shank_mask][:, sort_idx]\n",
    "    names_shank = electrode_names[shank_mask][sort_idx]\n",
    "    chan_shank = open_ephys_names[shank_mask][sort_idx]\n",
    "    bad_shank = bad_channels[shank_mask][sort_idx]\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for i in range(data_shank.shape[1]):\n",
    "        y = data_shank[:, i] + i * offset\n",
    "        plt.axhline(i * offset, color=\"gray\", linestyle=\"--\", lw=0.5)\n",
    "        alpha = 0.3 if bad_shank[i] else 1.0\n",
    "        plt.plot(t, y, lw=0.6, alpha=alpha)\n",
    "        label = f\"{names_shank[i]} - {chan_shank[i]}\"\n",
    "        plt.text(\n",
    "            t[-1] + 0.001 * (t[-1] - t[0]), i * offset, label, va=\"bottom\", fontsize=9\n",
    "        )\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Voltage (µV, offset by 1000)\")\n",
    "    plt.title(f\"{e_series.name} — Shank {shank}\")\n",
    "    plt.xlim(t[0], t[0] + duration * 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5517f",
   "metadata": {},
   "source": [
    "## Plot preprocessed ElectricalSeries\n\nThis plots a chunk of `recording = sgs.SpikeSortingRecording.get_recording(spikesorting_group_key)` for each SortGroup.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9826d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spikesorting_group_key in group_keys:\n",
    "    recording = sgs.SpikeSortingRecording.get_recording(spikesorting_group_key)\n",
    "\n",
    "    num_channels = recording.get_num_channels()\n",
    "    channel_ids = recording.get_channel_ids()\n",
    "    timestamps = recording.get_times()\n",
    "\n",
    "    data_shank = recording.get_traces(start_frame=start_idx, end_frame=end_idx)\n",
    "    time_axis = timestamps[start_idx:end_idx]\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for i in range(num_channels):\n",
    "        y = data_shank[:, i] + i * offset\n",
    "        plt.axhline(i * offset, color=\"gray\", linestyle=\"--\", lw=0.5)\n",
    "        plt.plot(t, y, lw=0.6, alpha=alpha)\n",
    "        label = f\"{channel_ids[i]}\"\n",
    "        plt.text(\n",
    "            t[-1] + 0.001 * (t[-1] - t[0]), i * offset, label, va=\"bottom\", fontsize=9\n",
    "        )\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Voltage (µV, offset by 1000)\")\n",
    "    plt.title(f\"Recording ID {spikesorting_group_key['recording_id']}\")\n",
    "    plt.xlim(t[0], t[0] + duration * 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348334fa",
   "metadata": {},
   "source": [
    "## Artifact Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74415172-f2da-4fd3-ab43-01857d682b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_keys)\n\nartifact_detection_keys = []\n\nfor group_key in group_keys:\n    key = {\n        \"recording_id\": group_key[\"recording_id\"],\n        \"artifact_param_name\": artifact_param_name,\n    }\n    # Insert into the selection table\n    sgs.ArtifactDetectionSelection.insert_selection(key)\n\n    # Grab the primary key (artifact_id) and add to our list so we can insert into ArtifactDetection\n    artifact_detection_keys.append((sgs.ArtifactDetectionSelection & key).fetch1(\"KEY\"))\n\n# Look at everything we inserted!\ndisplay(sgs.ArtifactDetectionSelection() & artifact_detection_keys)\n\nprint(\"Artifact detection keys:\")\nprint(artifact_detection_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bb031",
   "metadata": {},
   "source": [
    "Now call the `populate` method of `ArtifactDetection`.\n\nInstead of just calling `sgs.ArtifactDetection.populate(artifact_detection_keys)` with all artifact_detection_keys, we only populate the missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate ArtifactDetection for all artifact_detection_keys (ignoring ones already populated)\n",
    "\n",
    "# Print the set of all possible keys\n",
    "print(f\"There are {len(artifact_detection_keys)} keys: {artifact_detection_keys}\")\n",
    "\n",
    "# Get the set of already-populated keys in ArtifactDetection\n",
    "existing_keys = (sgs.ArtifactDetection & artifact_detection_keys).fetch(\n",
    "    \"KEY\", as_dict=True\n",
    ")\n",
    "print(\n",
    "    f\"There are {len(existing_keys)} already in in ArtifactDetection: {existing_keys}\"\n",
    ")\n",
    "\n",
    "# Find missing keys\n",
    "missing_keys = [key for key in artifact_detection_keys if key not in existing_keys]\n",
    "print(f\"There are {len(missing_keys)} missing keys: {missing_keys}\")\n",
    "\n",
    "# Populate only missing entries\n",
    "if missing_keys:\n",
    "    sgs.ArtifactDetection().populate(missing_keys)\n",
    "else:\n",
    "    print(\"All group keys already populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything worked!\ndisplay(sgs.ArtifactDetection() & artifact_detection_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee9ca19",
   "metadata": {},
   "source": [
    "## Run Spike Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdadbb",
   "metadata": {},
   "source": [
    "The spike sorting pipeline is powered by `spikeinterface`, a community-developed Python package that enables one to easily apply multiple spike sorters to a single recording. Some spike sorters have special requirements, such as GPU. Others need to be installed separately from spyglass. In the Frank lab, we have been using `mountainsort4`, though the pipeline have been tested with `mountainsort5`, `kilosort2_5`, `kilosort3`, and `ironclust` as well.\n\nWhen using `mountainsort5`, make sure to run `pip install mountainsort5`. `kilosort2_5`, `kilosort3`, and `ironclust` are MATLAB-based, but we can run these without having to install MATLAB thanks to `spikeinterface`. It does require downloading additional files (as singularity containers) so make sure to do `pip install spython`. These sorters also require GPU access, so also do ` pip install cuda-python` (and make sure your computer does have a GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34246883-9dc4-43c5-a438-009215a3a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into SpikeSortingSelection\n",
    "\n",
    "spike_sorting_keys = []\n",
    "\n",
    "for group_key in group_keys:\n",
    "    # Sometimes not all of these correctly populated in ArtifactDetectionSelection but we want to move forward anyway.\n",
    "    # So we do a check that the interval list actually exists so we can move forward with the ones that did\n",
    "    art_id = (\n",
    "        sgs.ArtifactDetectionSelection & {\"recording_id\": group_key[\"recording_id\"]}\n",
    "    ).fetch1(\"artifact_id\")\n",
    "    interval_list_entry = sgc.IntervalList() & {\"interval_list_name\": str(art_id)}\n",
    "    if len(interval_list_entry.fetch()) == 0:\n",
    "        print(f\"No interval list entry for {art_id}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    ss_key = {\n",
    "        \"recording_id\": group_key[\"recording_id\"],\n",
    "        \"sorter\": sorter,\n",
    "        \"nwb_file_name\": nwb_file_name,\n",
    "        \"interval_list_name\": str(art_id),\n",
    "        \"sorter_param_name\": sorter_param_name,\n",
    "    }\n",
    "    # Insert into the selection table\n",
    "    sgs.SpikeSortingSelection.insert_selection(ss_key)\n",
    "\n",
    "    # Grab the primary key (sorting_id) and add to our list so we can insert into SpikeSorting\n",
    "    spike_sorting_keys.append((sgs.SpikeSortingSelection & ss_key).proj().fetch1(\"KEY\"))\n",
    "\n",
    "# Look at everything we inserted!\n",
    "display(sgs.SpikeSortingSelection() & spike_sorting_keys)\n",
    "\n",
    "print(\"Spike sorting keys:\")\n",
    "print(spike_sorting_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82875015",
   "metadata": {},
   "source": [
    "Now call the `populate` method of `SpikeSorting`.\n\nInstead of just calling `sgs.SpikeSorting.populate(spike_sorting_keys)` with all spike_sorting_keys, we only populate the missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate SpikeSorting for all spike_sorting_keys (ignoring ones already populated)\n",
    "\n",
    "# Print the set of all possible keys\n",
    "print(f\"There are {len(spike_sorting_keys)} keys: {spike_sorting_keys}\")\n",
    "\n",
    "# Get the set of already-populated keys in sgs.SpikeSorting\n",
    "existing_keys = (sgs.SpikeSorting & spike_sorting_keys).fetch(\"KEY\", as_dict=True)\n",
    "print(f\"There are {len(existing_keys)} already in in sgs.SpikeSorting: {existing_keys}\")\n",
    "\n",
    "# Find missing keys\n",
    "missing_keys = [key for key in spike_sorting_keys if key not in existing_keys]\n",
    "print(f\"There are {len(missing_keys)} missing keys: {missing_keys}\")\n",
    "\n",
    "# Populate only missing entries\n",
    "if missing_keys:\n",
    "    sgs.SpikeSorting.populate(missing_keys)\n",
    "else:\n",
    "    print(\"All group keys already populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab079f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything worked!\ndisplay(sgs.SpikeSorting() & spike_sorting_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1e621",
   "metadata": {},
   "source": [
    "The spike sorting results (spike times of detected units) are saved in an NWB file. We can access this in two ways. First, we can access it via the `fetch_nwb` method, which allows us to directly access the spike times saved in the `units` table of the NWB file. Second, we can access it as a `spikeinterface.NWBSorting` object. This allows us to take advantage of the rich APIs of `spikeinterface` to further analyze the sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss_key in spike_sorting_keys:\n    sorting_nwb = (sgs.SpikeSorting & ss_key).fetch_nwb()\n    sorting_si = sgs.SpikeSorting.get_sorting(ss_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db328eb1",
   "metadata": {},
   "source": [
    "Note that the spike times of `fetch_nwb` is in units of seconds aligned with the timestamps of the recording. The spike times of the `spikeinterface.NWBSorting` object is in units of samples (as is generally true for sorting objects in `spikeinterface`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6c183",
   "metadata": {},
   "source": [
    "## Automatic Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8fcaa0-9dd7-4870-9f5b-be039e3579cc",
   "metadata": {},
   "source": [
    "Next step is to curate the results of spike sorting. This is often necessary because spike sorting algorithms are not perfect;\nthey often return clusters that are clearly not biological in origin, and sometimes oversplit clusters that should have been merged.\nWe have two main ways of curating spike sorting: by computing quality metrics followed by thresholding, and manually applying curation labels.\nTo do either, we first insert the spike sorting to `CurationV1` using `insert_curation` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245eec9-3fba-4071-b58b-eec6d9345532",
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key_list = []\n",
    "\n",
    "for ss_key in spike_sorting_keys:\n",
    "\n",
    "    # Check if this sorting_id has already been inserted with curation_id=1\n",
    "    initial_curation_key = {\"sorting_id\": str(ss_key[\"sorting_id\"]), \"curation_id\": 0}\n",
    "    initial_curation_entry = sgs.CurationV1() & initial_curation_key\n",
    "\n",
    "    # If it hasn't been inserted yet, insert into the curation table\n",
    "    if len(initial_curation_entry.fetch()) == 0:\n",
    "        sgs.CurationV1.insert_curation(\n",
    "            sorting_id=str(ss_key[\"sorting_id\"]),\n",
    "            description=\"initial automatic curation\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Entry for {initial_curation_key} already exists in sgs.CurationV1\")\n",
    "\n",
    "    curation_key_list.append(initial_curation_key)\n",
    "\n",
    "# Look at everything we inserted!\n",
    "display(sgs.CurationV1() & curation_key_list)\n",
    "\n",
    "print(\"Curation keys (initial automatic curation):\")\n",
    "print(curation_key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97317b6f-a40a-4f84-8042-4361064f010a",
   "metadata": {},
   "source": [
    "We will first do an automatic curation based on quality metrics. Under the hood, this part again makes use of `spikeinterface`. Some of the quality metrics that we often compute are the nearest neighbor isolation and noise overlap metrics, as well as SNR and ISI violation rate. For computing some of these metrics, the waveforms must be extracted and projected onto a feature space. Thus here we set the parameters for waveform extraction as well as how to curate the units based on these metrics (e.g. if `nn_noise_overlap` is greater than 0.1, mark as `noise`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207abda-ea84-43af-97d4-e5be3464d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_curation_keys = []\n",
    "for ss_key in spike_sorting_keys:\n",
    "    mc_key = {\n",
    "        \"sorting_id\": str(ss_key[\"sorting_id\"]),\n",
    "        \"curation_id\": 0,\n",
    "        \"waveform_param_name\": waveform_param_name,\n",
    "        \"metric_param_name\": metric_param_name,\n",
    "        \"metric_curation_param_name\": metric_curation_param_name,\n",
    "    }\n",
    "\n",
    "    # Insert into selection table\n",
    "    sgs.MetricCurationSelection.insert_selection(mc_key)\n",
    "\n",
    "    # Grab the primary key (metric_curation_id) and add to our list so we can insert into MetricCuration\n",
    "    metric_curation_keys.append((sgs.MetricCurationSelection & mc_key).fetch1(\"KEY\"))\n",
    "\n",
    "# Look at everything we inserted!\n",
    "display(sgs.MetricCurationSelection() & metric_curation_keys)\n",
    "\n",
    "print(\"Metric curation keys:\")\n",
    "print(metric_curation_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e10aa8",
   "metadata": {},
   "source": [
    "Now call the `populate` method of `MetricCuration`.\n\nInstead of just calling `sgs.MetricCuration.populate(metric_curation_keys)` with all metric_curation_keys, we only populate the missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f5725-4fd1-42ea-a1d4-590bd1353d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate MetricCuration for all metric_curation_keys (ignoring ones already populated)\n",
    "\n",
    "# Print the set of all possible keys\n",
    "print(f\"There are {len(metric_curation_keys)} keys: {metric_curation_keys}\")\n",
    "\n",
    "# Get the set of already-populated keys in sgs.MetricCuration\n",
    "existing_keys = (sgs.MetricCuration & metric_curation_keys).fetch(\"KEY\", as_dict=True)\n",
    "print(f\"There are {len(existing_keys)} already in in MetricCuration: {existing_keys}\")\n",
    "\n",
    "# Find missing keys\n",
    "missing_keys = [key for key in metric_curation_keys if key not in existing_keys]\n",
    "print(f\"There are {len(missing_keys)} missing keys: {missing_keys}\")\n",
    "\n",
    "# Populate only missing entries\n",
    "if missing_keys:\n",
    "    sgs.MetricCuration().populate(missing_keys)\n",
    "else:\n",
    "    print(\"All group keys already populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e95f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything worked!\ndisplay(sgs.MetricCuration() & metric_curation_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f354bf-0bfa-4148-9c5d-c5593f3f3915",
   "metadata": {},
   "source": [
    "To do another round of curation, fetch the relevant info and insert back into CurationV1 using `insert_curation`.\n\n\nBecause this is the second round, we have `curation_id=1` and `parent_curation_id=0` (to match the `curation_id=0` of the first round we inserted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ba8c0-560e-471b-9eaf-5924f6051faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key_list_round2 = []\n",
    "\n",
    "for mc_key in metric_curation_keys:\n",
    "\n",
    "    # Check if this sorting_id has already been inserted with curation_id=1\n",
    "    round_2_key = {\n",
    "        \"sorting_id\": str((sgs.MetricCurationSelection & mc_key).fetch1(\"sorting_id\")),\n",
    "        \"curation_id\": 1,\n",
    "    }\n",
    "    round_2_curation_entry = sgs.CurationV1() & round_2_key\n",
    "\n",
    "    # If it hasn't been inserted yet, insert into the curation table for a second round\n",
    "    if len(round_2_curation_entry.fetch()) == 0:\n",
    "        labels = sgs.MetricCuration.get_labels(mc_key)\n",
    "        merge_groups = sgs.MetricCuration.get_merge_groups(mc_key)\n",
    "        metrics = sgs.MetricCuration.get_metrics(mc_key)\n",
    "        sgs.CurationV1.insert_curation(\n",
    "            sorting_id=(sgs.MetricCurationSelection & mc_key).fetch1(\"sorting_id\"),\n",
    "            parent_curation_id=0,\n",
    "            labels=labels,\n",
    "            merge_groups=merge_groups,\n",
    "            metrics=metrics,\n",
    "            description=\"after metric curation\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Entry for {round_2_key} already exists in sgs.CurationV1\")\n",
    "\n",
    "    curation_key_list_round2.append(round_2_key)\n",
    "\n",
    "# Look at everything we inserted!\n",
    "display(sgs.CurationV1() & curation_key_list_round2)\n",
    "\n",
    "print(\"Curation keys (after metric curation):\")\n",
    "print(curation_key_list_round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8742e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined output of both rounds of curation!\ndisplay(sgs.CurationV1() & (curation_key_list_round2 + curation_key_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627274b",
   "metadata": {},
   "source": [
    "## For now, we skip manual curation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6aff5-7020-40d6-832f-006d66d54a7e",
   "metadata": {},
   "source": [
    "## Insert into merge table for downstream usage \n\nRegardless of Curation method used, to make use of spikeorting results in downstream pipelines like Decoding, we will need to insert it into the `SpikeSortingOutput` merge table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ecb19-7d8d-4db6-be71-c0ed66e2b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.spikesorting.spikesorting_merge import SpikeSortingOutput\n",
    "\n",
    "# Insert the second round curation results into the merge table\n",
    "for key in curation_key_list_round2:\n",
    "\n",
    "    # Check if this key has already been inserted\n",
    "    merge_insert_key = (sgs.CurationV1 & key).fetch(\"KEY\", as_dict=True)\n",
    "    merge_entry = SpikeSortingOutput() & merge_insert_key\n",
    "\n",
    "    # If it hasn't been inserted yet, insert it\n",
    "    if len(merge_entry.fetch()) == 0:\n",
    "        SpikeSortingOutput.insert(merge_insert_key, part_name=\"CurationV1\")\n",
    "    else:\n",
    "        print(f\"Entry for {merge_insert_key} already exists in SpikeSortingOutput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd3f5b",
   "metadata": {},
   "source": [
    "Look at our entries in `SpikeSortingOutput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c535873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we just restrict on nwb file name and curation id\n",
    "# We could also restrict on sorter, interval_list_name, etc\n",
    "selection_key = {\"nwb_file_name\": nwb_file_name, \"curation_id\": 1}\n",
    "merge_ids = SpikeSortingOutput().get_restricted_merge_ids(\n",
    "    key=selection_key, sources=\"v1\", as_dict=True\n",
    ")\n",
    "\n",
    "# View all of our entries in the table\n",
    "display(SpikeSortingOutput() & merge_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2f194",
   "metadata": {},
   "source": [
    "---------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112c174",
   "metadata": {},
   "source": [
    "# Decode from sorted spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140bb106",
   "metadata": {},
   "source": [
    "The elements we will need to decode with sorted spikes are:\n- `PositionGroup`\n- `SortedSpikesGroup`\n- `DecodingParameters`\n- `encoding_interval`\n- `decoding_interval`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.spikesorting.spikesorting_merge import SpikeSortingOutput\n",
    "from spyglass.spikesorting.analysis.v1.group import SortedSpikesGroup\n",
    "import spyglass.spikesorting.v1 as sgs\n",
    "\n",
    "sorter_keys = {\n",
    "    \"nwb_file_name\": nwb_file_name,\n",
    "    \"sorter\": sorter,\n",
    "    \"curation_id\": 1,\n",
    "}\n",
    "# Check out the set of sorting we'll use\n",
    "display(\n",
    "    (sgs.SpikeSortingSelection & sorter_keys)\n",
    "    * (SpikeSortingOutput.CurationV1 & sorter_keys)\n",
    ")\n",
    "\n",
    "# Get the merge_ids for the selected sorting\n",
    "spikesorting_merge_ids = SpikeSortingOutput().get_restricted_merge_ids(\n",
    "    sorter_keys, restrict_by_artifact=False\n",
    ")\n",
    "print(\n",
    "    f\"We have {len(spikesorting_merge_ids)} merge_ids for this group {spikesorting_merge_ids}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sorted spikes group\n",
    "ss_group_name = \"sorted_spikes_group\"\n",
    "\n",
    "group_entry = SortedSpikesGroup & {\n",
    "    \"nwb_file_name\": nwb_file_name,\n",
    "    \"sorted_spikes_group_name\": ss_group_name,\n",
    "}\n",
    "\n",
    "# If the group hasn't been created yet, create it\n",
    "if len(group_entry.fetch()) == 0:\n",
    "    SortedSpikesGroup().create_group(\n",
    "        group_name=ss_group_name,\n",
    "        nwb_file_name=nwb_file_name,\n",
    "        keys=[\n",
    "            {\"spikesorting_merge_id\": merge_id} for merge_id in spikesorting_merge_ids\n",
    "        ],\n",
    "        unit_filter_params_name=unit_filter_params_name,\n",
    "    )\n",
    "else:\n",
    "    print(f\"SortedSpikesGroup already exists!\")\n",
    "\n",
    "# Check out the new group\n",
    "display(\n",
    "    SortedSpikesGroup\n",
    "    & {\"nwb_file_name\": nwb_file_name, \"sorted_spikes_group_name\": ss_group_name}\n",
    ")\n",
    "\n",
    "# And look at the sorting within the group\n",
    "display(\n",
    "    SortedSpikesGroup.Units\n",
    "    & {\n",
    "        \"nwb_file_name\": nwb_file_name,\n",
    "        \"sorted_spikes_group_name\": ss_group_name,\n",
    "        \"unit_filter_params_name\": unit_filter_params_name,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dabc1c",
   "metadata": {},
   "source": [
    "## Grouping Position Data\n\nNote that we can use the `upsample_rate` parameter to define the rate to which position data will be upsampled to to for decoding in Hz. This is useful if we want to decode at a finer time scale than the position data sampling frequency. In practice, a value of 500Hz is used in many analyses. Skipping or providing a null value for this parameter will default to using the position sampling rate.\n\nYou will also want to specify the name of the position variables if they are different from the default names. The default names are `position_x` and `position_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.position import PositionOutput\n",
    "import spyglass.position as sgp\n",
    "\n",
    "pos_group_name = \"sorted_spikes_pos_group\"\n",
    "\n",
    "# Set up position key for position we want to use to decode\n",
    "position_selection_key = {\n",
    "    \"nwb_file_name\": nwb_file_name,\n",
    "    \"interval_list_name\": \"pos 0 valid times\",  # Berke lab has only one epoch, so this is always our interval list name\n",
    "    \"trodes_pos_params_name\": trodes_pos_params_name,\n",
    "}\n",
    "\n",
    "# Insert into selection table\n",
    "sgp.v1.TrodesPosSelection.insert1(\n",
    "    position_selection_key,\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "\n",
    "# Fetch the primary key so we can populate the the position table\n",
    "# (it's actually the same as position_selection_key so we could have just used that)\n",
    "position_key = (sgp.v1.TrodesPosSelection() & position_selection_key).fetch1(\"KEY\")\n",
    "pos_entry = PositionOutput.TrodesPosV1() & position_key\n",
    "\n",
    "# If we don't have a position entry with our decoding parameters yet, insert it\n",
    "if len(pos_entry.fetch()) == 0:\n",
    "    sgp.v1.TrodesPosV1.populate(position_key)\n",
    "else:\n",
    "    print(f\"Entry for {position_key} already exists in sgp.v1.TrodesPosV1\")\n",
    "\n",
    "# Look at it!\n",
    "display(PositionOutput.TrodesPosV1 & position_key)\n",
    "\n",
    "print(f\"Pos selection key: {position_selection_key}\")\n",
    "print(f\"Pos key: {position_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4494194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.core import PositionGroup\n",
    "\n",
    "position_merge_ids = (PositionOutput.TrodesPosV1 & position_key).fetch(\"merge_id\")\n",
    "\n",
    "pos_group_entry = PositionGroup & {\n",
    "    \"nwb_file_name\": nwb_file_name,\n",
    "    \"position_group_name\": pos_group_name,\n",
    "}\n",
    "\n",
    "# If we don't have a position group yet, create it!\n",
    "if len(pos_group_entry.fetch()) == 0:\n",
    "    PositionGroup().create_group(\n",
    "        nwb_file_name=nwb_file_name,\n",
    "        group_name=pos_group_name,\n",
    "        keys=[{\"pos_merge_id\": merge_id} for merge_id in position_merge_ids],\n",
    "        upsample_rate=500,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Position group already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d609823",
   "metadata": {},
   "source": [
    "## Decoding\n\nNow we can decode the position using the sorted spikes using the `SortedSpikesDecodingSelection` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cf161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding import SortedSpikesDecodingSelection\n",
    "\n",
    "selection_key = {\n",
    "    \"sorted_spikes_group_name\": ss_group_name,\n",
    "    \"unit_filter_params_name\": unit_filter_params_name,\n",
    "    \"position_group_name\": pos_group_name,\n",
    "    \"decoding_param_name\": decoding_param_name,\n",
    "    \"nwb_file_name\": nwb_file_name,\n",
    "    \"encoding_interval\": \"00_r1\",  # to encode using the entire session, this is always our interval list name\n",
    "    \"decoding_interval\": \"epoch0_block1\",\n",
    "    \"estimate_decoding_params\": False,\n",
    "}\n",
    "\n",
    "SortedSpikesDecodingSelection.insert1(\n",
    "    selection_key,\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee943321",
   "metadata": {},
   "source": [
    "Run decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.sorted_spikes import SortedSpikesDecodingV1\n",
    "\n",
    "decoding_entry = SortedSpikesDecodingV1 & selection_key\n",
    "\n",
    "# Run the decoding if we don't have output yet\n",
    "if len(decoding_entry.fetch()) == 0:\n",
    "    SortedSpikesDecodingV1.populate(selection_key)\n",
    "else:\n",
    "    print(\"Decoding entry already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c17fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.decoding_merge import DecodingOutput\n\ndisplay(DecodingOutput.SortedSpikesDecodingV1 & selection_key)\n\n# Fetch results\ndecoding_results = (SortedSpikesDecodingV1 & selection_key).fetch_results()\ndisplay(decoding_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7939d",
   "metadata": {},
   "source": [
    "### Plot place fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3200e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.decoding_merge import DecodingOutput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_firing_rate = 15  # spikes/s\n",
    "show_colorbar = False\n",
    "\n",
    "# Fetch classifier\n",
    "classifier = (SortedSpikesDecodingV1 & selection_key).fetch_model()\n",
    "fs = classifier.sampling_frequency\n",
    "\n",
    "# Fetch place fields and reshape\n",
    "place_fields = classifier.encoding_model_[(\"\", 0)][\"place_fields\"]  # units, place_bins\n",
    "print(place_fields.shape)\n",
    "place_fields = place_fields.reshape(\n",
    "    (-1, *classifier.environments[0].centers_shape_)\n",
    ")  # units, x, y\n",
    "print(place_fields.shape)\n",
    "\n",
    "# Set up subplots\n",
    "n_units = place_fields.shape[0]\n",
    "\n",
    "n_cols = 10  # number of columns in the grid\n",
    "n_rows = int(np.ceil(n_units / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "\n",
    "# Plot the place field for each unit\n",
    "mappable = None\n",
    "for i in range(n_units):\n",
    "    ax = axes.flat[i]\n",
    "    m = ax.pcolormesh(\n",
    "        classifier.environments[0].edges_[0],\n",
    "        classifier.environments[0].edges_[1],\n",
    "        place_fields[i].T * fs,\n",
    "        vmin=0,\n",
    "        vmax=max_firing_rate,\n",
    "    )\n",
    "\n",
    "    # Optional add colorbar\n",
    "    if show_colorbar:\n",
    "        fig.colorbar(m, ax=ax, label=\"spikes/s\")\n",
    "\n",
    "    ax.set_title(f\"Unit {i}\", fontsize=8)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# Turn off any unused subplots\n",
    "for j in range(n_units, n_rows * n_cols):\n",
    "    axes.flat[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85d7ef",
   "metadata": {},
   "source": [
    "## Create a figurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from non_local_detector.visualization import (\n#     create_interactive_2D_decoding_figurl,\n# )\n\n# (\n#     position_info,\n#     position_variable_names,\n# ) = SortedSpikesDecodingV1.fetch_position_info(selection_key)\n# results_time = decoding_results.acausal_posterior.isel(intervals=0).time.values\n# position_info = position_info.loc[results_time[0] : results_time[-1]]\n\n# env = SortedSpikesDecodingV1.fetch_environments(selection_key)[0]\n# spike_times = SortedSpikesDecodingV1.fetch_spike_data(selection_key)\n\n# url = create_interactive_2D_decoding_figurl(\n#     position_time=position_info.index.to_numpy(),\n#     position=position_info[position_variable_names],\n#     env=env,\n#     results=decoding_results,\n#     posterior=decoding_results.acausal_posterior.isel(intervals=0)\n#     .unstack(\"state_bins\")\n#     .sum(\"state\"),\n#     spike_times=spike_times,\n#     head_dir=position_info[\"orientation\"],\n#     speed=position_info[\"speed\"],\n# )\n# url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47716ad",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ce599",
   "metadata": {},
   "source": [
    "# Evaluate!\n\nCheck out our units and plot ISI and place info, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f544a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set up a dataframe with one row per unit\n",
    "all_units = pd.DataFrame()\n",
    "for curation_key in curation_key_list_round2:\n",
    "    units_for_this_sortgroup = sgs.CurationV1().get_sorting(\n",
    "        curation_key, as_dataframe=True\n",
    "    )\n",
    "    all_units = pd.concat((all_units, units_for_this_sortgroup))\n",
    "\n",
    "# Exclude units with \"noise\" in curation_label\n",
    "good_units = all_units[~all_units[\"curation_label\"].apply(lambda x: \"noise\" in x)]\n",
    "display(good_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e44a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean isi for each unit\n",
    "good_units[\"mean_isi\"] = good_units[\"spike_times\"].apply(\n",
    "    lambda s: np.diff(np.sort(s)).mean() if len(s) > 1 else np.nan\n",
    ")\n",
    "display(good_units)\n",
    "\n",
    "# Plot histogram of ISIs\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(good_units[\"mean_isi\"].dropna(), bins=50)\n",
    "plt.xlabel(\"Mean ISI (s)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of mean ISIs\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histpgram of spike counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(good_units[\"num_spikes\"].dropna(), bins=50)\n",
    "plt.xlabel(\"Number of spikes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of spike counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1869f",
   "metadata": {},
   "source": [
    "### Plot ISI histograms for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_isi = 0.15\n",
    "\n",
    "# Set up subplots\n",
    "n_units = len(good_units)\n",
    "\n",
    "n_cols = 10  # number of columns in the grid\n",
    "n_rows = int(np.ceil(n_units / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "\n",
    "# Plot the ISI histogram for each unit\n",
    "for plot_idx, (i, row) in enumerate(good_units.iterrows()):\n",
    "    ax = axes.flat[plot_idx]\n",
    "    spikes = np.sort(row[\"spike_times\"])\n",
    "    isi = np.diff(spikes)\n",
    "    ax.hist(isi[isi <= max_isi], bins=50)\n",
    "    ax.set_title(f\"Unit {plot_idx}\", fontsize=8)\n",
    "    ax.set_xlim(0, max_isi)\n",
    "\n",
    "# Turn off any unused subplots\n",
    "for j in range(n_units, n_rows * n_cols):\n",
    "    axes.flat[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the position info we used for decoding\n",
    "pos_merge_id = (PositionOutput.TrodesPosV1() & position_key).fetch(\"KEY\")\n",
    "pos_df = (PositionOutput & pos_merge_id).fetch1_dataframe()\n",
    "display(pos_df)\n",
    "\n",
    "# Make it an array with shape (n_time, 2)\n",
    "positions = pos_df[[\"position_x\", \"position_y\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get timestamps of position used for decoding\n",
    "timestamps = pos_df.index\n",
    "# Extend the last bin slightly to catch trailing spikes\n",
    "timestamps = np.append(timestamps, timestamps[-1] + np.diff(timestamps[-2:]).mean())\n",
    "\n",
    "# Set up an array with shape (n_time, n_neurons)\n",
    "# that is a binary indicator of whether there was a spike in a given time bin for a given neuron\n",
    "n_time = len(timestamps) - 1\n",
    "n_neurons = len(good_units)\n",
    "spikes = np.zeros((n_time, n_neurons), dtype=int)\n",
    "\n",
    "for i, spike_times in enumerate(good_units[\"spike_times\"]):\n",
    "    spike_counts, _ = np.histogram(spike_times, bins=timestamps)\n",
    "    spikes[:, i] = (spike_counts > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that this worked.\n",
    "spikes_per_neuron = spikes.sum(axis=0)\n",
    "original_counts = good_units[\"spike_times\"].apply(len).to_numpy()\n",
    "print(spikes_per_neuron)\n",
    "print(original_counts)\n",
    "print(original_counts - spikes_per_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516a7b6",
   "metadata": {},
   "source": [
    "### Plot spike locations for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_time, n_units = spikes.shape\n",
    "\n",
    "# Set up subplots\n",
    "n_cols = 10  # number of columns in the grid\n",
    "n_rows = int(np.ceil(n_units / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "\n",
    "# Plot the rat's position when the cell fires for each unit\n",
    "for i in range(n_units):\n",
    "    ax = axes.flat[i]\n",
    "    spike_mask = spikes[:, i].astype(bool)\n",
    "    pos_spike = positions[spike_mask]\n",
    "\n",
    "    ax.scatter(positions[:, 0], positions[:, 1], s=1, alpha=0.1, color=\"gray\")\n",
    "    ax.scatter(pos_spike[:, 0], pos_spike[:, 1], s=1, color=\"red\")\n",
    "    ax.set_title(f\"Unit {i}\", fontsize=8)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# Turn off any unused subplots\n",
    "for j in range(n_units, n_rows * n_cols):\n",
    "    axes.flat[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153876db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the relevant primary keys we created.\n\nprint(\"Keys for sgs.SpikeSortingRecording\")\nprint(group_keys)\ndisplay(sgs.SpikeSortingRecording & group_keys)\n\nprint(\"Keys for sgs.ArtifactDetection\")\nprint(artifact_detection_keys)\ndisplay(sgs.ArtifactDetection() & artifact_detection_keys)\n\nprint(\"Keys for sgs.SpikeSorting\")\nprint(spike_sorting_keys)\ndisplay(sgs.SpikeSorting() & spike_sorting_keys)\n\nprint(\"Keys for sgs.CurationV1 (initial round of curation)\")\nprint(curation_key_list)\ndisplay(sgs.CurationV1() & curation_key_list)\n\nprint(\"Keys for sgs.MetricCurationSelection\")\nprint(metric_curation_keys)\ndisplay(sgs.MetricCurationSelection() & metric_curation_keys)\n\nprint(\"Keys for sgs.CurationV1 (after metric curation)\")\nprint(curation_key_list_round2)\ndisplay(sgs.CurationV1() & curation_key_list_round2)\n\nprint(\"Keys for SpikeSortingOutput\")\nprint(merge_ids)\ndisplay(SpikeSortingOutput() & merge_ids)\n\nprint(\"Key for DecodingOutput.SortedSpikesDecodingV1\")\nprint(selection_key)\ndisplay(DecodingOutput.SortedSpikesDecodingV1 & selection_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyglass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}